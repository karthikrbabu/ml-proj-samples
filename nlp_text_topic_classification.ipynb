{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6t55qCf3Fsxa"
   },
   "source": [
    "# Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9-9G8z4Fsxd"
   },
   "source": [
    "Below is an example working with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Here we'll represent each document with a \"bag-of-words\" model. This makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUYW83LqFsxd"
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "#Karthik NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "#Kartihk UTILITIES\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ALNjGgCFsxg"
   },
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecYpcoxaFsxh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "test label shape: (677,)\n",
      "dev label shape: (676,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = int(len(newsgroups_test.target) / 2)\n",
    "test_data, test_labels = newsgroups_test.data[num_test:], newsgroups_test.target[num_test:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test], newsgroups_test.target[:num_test]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('test label shape:', test_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHTnOke6Fsxk"
   },
   "source": [
    "### Part 1:\n",
    "\n",
    "For each of the first 5 training examples, print the text of the message along with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8zhA06xFsxl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: comp.graphics\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Label: talk.religion.misc\n",
      "\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Label: sci.space\n",
      "\n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Label: alt.atheism\n",
      "\n",
      "I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Label: sci.space\n",
      "\n",
      "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P1(num_examples=5):\n",
    "## STUDENT START ###\n",
    "\n",
    "    #Built a Pandas DF with num_exampe entries\n",
    "    d = {'text': train_data[0:num_examples], 'label': train_labels[0:num_examples]}\n",
    "    train_examples = pd.DataFrame(data=d)\n",
    "    \n",
    "    #Iterate through rows and print data\n",
    "    for index, row in train_examples.iterrows():\n",
    "        print(\"Label: {0}\".format(newsgroups_train.target_names[row['label']]))\n",
    "        print()\n",
    "        print(row['text'])\n",
    "        print()\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()\n",
    "    \n",
    "    #Return dataframe of num_examples size\n",
    "    return train_examples\n",
    "    \n",
    "## STUDENT END ###\n",
    "p1_examples = P1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onfno6uHFsxm"
   },
   "source": [
    "### Part 2:\n",
    "\n",
    "Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyVwk5RvFsxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Matrix Shape:  (2034, 26879)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "PART A:\n",
      "\n",
      "Train Data Vocab Size:  26879\n",
      "\n",
      "Average Number of Non-zero Features  96.70599803343165\n",
      "\n",
      "Fraction of Non-zero Entries: 0.0035978272269590263\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "PART B:\n",
      "\n",
      "First feature: 00\n",
      "\n",
      "Last feature: zyxel\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "PART C:\n",
      "\n",
      "Custom Vocab:\n",
      "{'atheism': 0, 'graphics': 1, 'space': 2, 'religion': 3}\n",
      "\n",
      "Own Vocab Matrix Shape: (2034, 4)\n",
      "\n",
      "Own Vocab Matrix: [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]]\n",
      "\n",
      "Average Number of Non-zero Features  0.26843657817109146\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "PART D:\n",
      "\n",
      "Bigram + Trigram on Character Features\n",
      "Vocab Size:  35478\n",
      "\n",
      "Vocab Matrix Shape: (2034, 35478)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "PART E:\n",
      "\n",
      "min_df = 10 : Prune words that appear in fewer than 10 documents\n",
      "\n",
      "Vocab Size:  3064\n",
      "\n",
      "Vocab Matrix Shape: (2034, 3064)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "PART F:\n",
      "\n",
      "Dev Size vs. Train Size\n",
      "\n",
      "Dev Vocab Size:  16246\n",
      "\n",
      "Train Vocab Size:  26879\n",
      "\n",
      "Words in dev_data missing from train_data 4027 words\n",
      "\n",
      "This is 24.788% of the dev_data\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P2():\n",
    "    \n",
    "    # Could include this option to ignore basic stop words throughout the text\n",
    "    # stop_words='english'    \n",
    "    \n",
    "    global_vectorizer = CountVectorizer(lowercase=True)\n",
    "    # global_vectorizer = CountVectorizer(lowercase=True, token_pattern = \"^[^0-9]+$\")\n",
    "    global_vtrain = global_vectorizer.fit_transform(train_data)\n",
    "    sparse_matrix = global_vtrain.toarray()\n",
    "    \n",
    "    print (\"Train Data Matrix Shape:  {0}\".format(global_vtrain.shape))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    def a():\n",
    "        \n",
    "        print( \"PART A:\")\n",
    "        print()\n",
    "        \n",
    "        vocab_size = len(global_vectorizer.get_feature_names())\n",
    "        print (\"Train Data Vocab Size:  {0}\".format(vocab_size))\n",
    "        print()\n",
    "        \n",
    "        avg_non_zero_per_entry = global_vtrain.nnz/global_vtrain.shape[0]\n",
    "        print (\"Average Number of Non-zero Features  {0}\".format(avg_non_zero_per_entry))\n",
    "        print()        \n",
    "        \n",
    "        total_slots = global_vtrain.shape[0] * global_vtrain.shape[1]\n",
    "        full_slots = global_vtrain.nnz\n",
    "        ratio = full_slots/ total_slots\n",
    "        print (\"Fraction of Non-zero Entries: {0}\".format(ratio))\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    def b():\n",
    "        \n",
    "        print( \"PART B:\")\n",
    "        print()\n",
    "        \n",
    "        first_feature = global_vectorizer.get_feature_names()[0]\n",
    "        last_feature = global_vectorizer.get_feature_names()[-1]\n",
    "    \n",
    "        print (\"First feature: {0}\".format(first_feature))\n",
    "        print()\n",
    "        print (\"Last feature: {0}\".format(last_feature))\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def c():\n",
    "\n",
    "        print( \"PART C:\")\n",
    "        print()\n",
    "        \n",
    "\n",
    "        own_vocab = [\"atheism\", \"graphics\", \"space\", \"religion\"]\n",
    "        own_vectorizer = CountVectorizer(lowercase=True, vocabulary=own_vocab)\n",
    "        own_vtrain = own_vectorizer.fit_transform(train_data)\n",
    "        \n",
    "        print(\"Custom Vocab:\")\n",
    "        print(own_vectorizer.vocabulary_)\n",
    "        print()        \n",
    "        print (\"Own Vocab Matrix Shape: {0}\".format(own_vtrain.shape))\n",
    "        print()\n",
    "        print (\"Own Vocab Matrix: {0}\".format(own_vtrain.toarray()))\n",
    "        print()\n",
    "\n",
    "        avg_non_zero_per_entry = own_vtrain.nnz/own_vtrain.shape[0]\n",
    "        print (\"Average Number of Non-zero Features  {0}\".format(avg_non_zero_per_entry))\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()\n",
    "        \n",
    "\n",
    "    def d():\n",
    "        \n",
    "        print( \"PART D:\")\n",
    "        print()\n",
    "        print(\"Bigram + Trigram on Character Features\")\n",
    "        \n",
    "        vectorizer = CountVectorizer(lowercase=True, ngram_range= (2, 3), analyzer='char')\n",
    "        vtrain = vectorizer.fit_transform(train_data)        \n",
    "        \n",
    "        vocab_size = len(vectorizer.get_feature_names())\n",
    "        print (\"Vocab Size:  {0}\".format(vocab_size))\n",
    "        print()\n",
    "        print (\"Vocab Matrix Shape: {0}\".format(vtrain.shape))\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    def e():\n",
    "        \n",
    "        print( \"PART E:\")\n",
    "        print()\n",
    "        print(\"min_df = 10 : Prune words that appear in fewer than 10 documents\")\n",
    "        print()\n",
    "        \n",
    "        vectorizer = CountVectorizer(lowercase=True, min_df=10)\n",
    "        vtrain = vectorizer.fit_transform(train_data)        \n",
    "        \n",
    "        vocab_size = len(vectorizer.get_feature_names())\n",
    "        print (\"Vocab Size:  {0}\".format(vocab_size))\n",
    "        print()\n",
    "        print (\"Vocab Matrix Shape: {0}\".format(vtrain.shape))\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()\n",
    "        \n",
    "    def f():\n",
    "        \n",
    "        print( \"PART F:\")\n",
    "        print()\n",
    "        print(\"Dev Size vs. Train Size\")\n",
    "        print()\n",
    "        \n",
    "        dev_vectorizer = CountVectorizer(lowercase=True)\n",
    "        dev_vtrain = dev_vectorizer.fit_transform(dev_data)\n",
    "        \n",
    "        dev_vocab = set(dev_vectorizer.get_feature_names())\n",
    "        print (\"Dev Vocab Size:  {0}\".format(len(dev_vocab)))\n",
    "        print()\n",
    "        train_vocab = set(global_vectorizer.get_feature_names())\n",
    "        print (\"Train Vocab Size:  {0}\".format(len(train_vocab)))\n",
    "        print()\n",
    "        \n",
    "        print(\"Words in dev_data missing from train_data {0} words\".format(len(dev_vocab - train_vocab)))\n",
    "        print()\n",
    "        print(\"This is {0}% of the dev_data\".format(np.round(100*len(dev_vocab - train_vocab)/len(dev_vocab), 3)))\n",
    "        print()    \n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()        \n",
    "                \n",
    "    a()\n",
    "    b()\n",
    "    c()\n",
    "    d()\n",
    "    e()\n",
    "    f()\n",
    "        \n",
    "P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydjkRh6LFsxp"
   },
   "source": [
    "### Part 3:\n",
    "\n",
    "Use the default CountVectorizer options and report the f1 score (use metrics.f1_score with average=\"weighted\") for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
    "\n",
    "* Why doesn't nearest neighbors work well for this problem?\n",
    "* Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "* Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvhpODdWFsxp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN MODEL:\n",
      "\n",
      "Best k value on range [1-25] :  19\n",
      "\n",
      "\n",
      "F1 Score (Weighted) 0.4365794782510113\n",
      "\n",
      "Classification Report for KNearestNeighbors with neighbors=19\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.50      0.43       165\n",
      "           1       0.47      0.48      0.48       185\n",
      "           2       0.55      0.49      0.52       199\n",
      "           3       0.31      0.23      0.26       127\n",
      "\n",
      "    accuracy                           0.44       676\n",
      "   macro avg       0.43      0.42      0.42       676\n",
      "weighted avg       0.44      0.44      0.44       676\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      "MULTINOMIAL NAIVE BAYES MODEL:\n",
      "\n",
      "Best alpha value on range [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0] :  0.01\n",
      "\n",
      "\n",
      "F1 Score (Weighted) 0.7751663218544357\n",
      "\n",
      "Classification Report for MultinomialNB with alpha=0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.69       165\n",
      "           1       0.92      0.90      0.91       185\n",
      "           2       0.81      0.89      0.85       199\n",
      "           3       0.65      0.50      0.57       127\n",
      "\n",
      "    accuracy                           0.78       676\n",
      "   macro avg       0.76      0.75      0.76       676\n",
      "weighted avg       0.78      0.78      0.78       676\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      "LOGISTIC REGRESSION MODEL:\n",
      "\n",
      "Best Estimator LogisticRegression(C=0.615848211066026, solver='liblinear')\n",
      "\n",
      "Best \"C\" value on range [np.logspace(-4, 4, 20)] :  0.615848211066026\n",
      "\n",
      "\n",
      "F1 Score (Weighted) 0.7091615339971832\n",
      "\n",
      "Classification Report for LogisticRegression with C=0.615848211066026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.56      0.59       165\n",
      "           1       0.81      0.88      0.84       185\n",
      "           2       0.75      0.81      0.78       199\n",
      "           3       0.61      0.54      0.57       127\n",
      "\n",
      "    accuracy                           0.71       676\n",
      "   macro avg       0.70      0.69      0.69       676\n",
      "weighted avg       0.71      0.71      0.71       676\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      "Summary F1 Scores:\n",
      "{'knn': 0.4365794782510113, 'multi_nb_score': 0.7751663218544357, 'log_reg_score': 0.7091615339971832}\n"
     ]
    }
   ],
   "source": [
    "def P3():\n",
    "\n",
    "    #Common vectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    #TRAINING\n",
    "    sparse_train = vectorizer.fit_transform(train_data)\n",
    "    \n",
    "    #DEVELOPMENT\n",
    "    sparse_dev = vectorizer.transform(dev_data)\n",
    "    \n",
    "    \n",
    "    def knn():\n",
    "        \n",
    "        print()\n",
    "        print(\"KNN MODEL:\")\n",
    "        print()        \n",
    "        # Use GridSearch to try various values of hyper parameter \"k\"\n",
    "        n_neighbors = {'n_neighbors': list(range(1,25,2))}\n",
    "        knn_cv = GridSearchCV(estimator=KNeighborsClassifier(),\n",
    "                                   param_grid=n_neighbors,\n",
    "                                   cv=5,\n",
    "                                   return_train_score=True)\n",
    "\n",
    "        knn_cv.fit(sparse_train,train_labels)\n",
    "        best_k = knn_cv.best_params_['n_neighbors']\n",
    "        print(\"Best k value on range [1-25] :  {0}\".format(best_k))\n",
    "        print()\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "        knn.fit(sparse_train,train_labels)\n",
    "        train_predict = knn.predict(sparse_dev)\n",
    "        \n",
    "        \n",
    "        f1_score = metrics.f1_score(dev_labels, train_predict, average=\"weighted\")\n",
    "        print()\n",
    "        print(\"F1 Score (Weighted) {0}\".format(f1_score))\n",
    "        print()\n",
    "        print(\"Classification Report for KNearestNeighbors with neighbors={0}\".format(best_k))\n",
    "        print(classification_report(dev_labels,train_predict))        \n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()\n",
    "        return f1_score\n",
    "        \n",
    "        \n",
    "    def multi_naive_bayes():\n",
    "        \n",
    "        print()\n",
    "        print(\"MULTINOMIAL NAIVE BAYES MODEL:\")     \n",
    "        print()\n",
    "        # Use GridSearch to try various values of hyper parameter \"alpha\"\n",
    "        alphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "        multi_nb_cv = GridSearchCV(estimator=MultinomialNB(),\n",
    "                                   param_grid=alphas,\n",
    "                                   cv=5,\n",
    "                                   return_train_score=True)\n",
    "\n",
    "        multi_nb_cv.fit(sparse_train,train_labels)\n",
    "        best_alpha = multi_nb_cv.best_params_['alpha']\n",
    "        print(\"Best alpha value on range [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0] :  {0}\".format(best_alpha))\n",
    "        print()\n",
    "        \n",
    "        multi_nb = MultinomialNB(alpha=best_alpha)\n",
    "        multi_nb.fit(sparse_train,train_labels)\n",
    "        train_predict = multi_nb.predict(sparse_dev)\n",
    "        f1_score = metrics.f1_score(dev_labels, train_predict, average=\"weighted\")\n",
    "        print()\n",
    "        print(\"F1 Score (Weighted) {0}\".format(f1_score))\n",
    "        print()\n",
    "        print(\"Classification Report for MultinomialNB with alpha={0}\".format(best_alpha))\n",
    "        print(classification_report(dev_labels,train_predict))        \n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print() \n",
    "        return f1_score\n",
    "\n",
    "    def log_reg():\n",
    "\n",
    "        \n",
    "        print()\n",
    "        print(\"LOGISTIC REGRESSION MODEL:\")     \n",
    "        print()        \n",
    "        \n",
    "        param_grid = [\n",
    "            {'penalty' : ['l2'],\n",
    "             'multi_class': [\"auto\"],\n",
    "            'C' : np.logspace(-4, 4, 20),\n",
    "            'solver' : ['liblinear'],\n",
    "            'max_iter' : [100, 1000,2500, 5000]\n",
    "            }\n",
    "        ]        \n",
    "        \n",
    "        # Use GridSearch to try various values of hyper parameter \"c\"\n",
    "        # note: liblinear cannot be parallelized\n",
    "        log_reg_cv = GridSearchCV(estimator=LogisticRegression(),\n",
    "                                  param_grid=param_grid,\n",
    "                                  n_jobs=-1,\n",
    "                                  cv=5)\n",
    "\n",
    "        log_reg_cv.fit(sparse_train,train_labels)\n",
    "        \n",
    "        print(\"Best Estimator {0}\".format(log_reg_cv.best_estimator_))\n",
    "        print()\n",
    "        best_c = log_reg_cv.best_params_['C']\n",
    "        print(\"Best \\\"C\\\" value on range [np.logspace(-4, 4, 20)] :  {0}\".format(best_c))\n",
    "        print()\n",
    "        \n",
    "        log_reg = LogisticRegression(C=best_c, multi_class=\"auto\", solver='liblinear')\n",
    "        log_reg.fit(sparse_train,train_labels)\n",
    "        train_predict = log_reg.predict(sparse_dev)\n",
    "        f1_score = metrics.f1_score(dev_labels, train_predict, average=\"weighted\")\n",
    "        print()\n",
    "        print(\"F1 Score (Weighted) {0}\".format(f1_score))\n",
    "        print()\n",
    "        print(\"Classification Report for LogisticRegression with C={0}\".format(best_c))\n",
    "        print(classification_report(dev_labels,train_predict))        \n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()\n",
    "        \n",
    "    \n",
    "        class_coeffs = log_reg.coef_\n",
    "        \n",
    "        output = {}\n",
    "        \n",
    "        for index,class_weights in enumerate(class_coeffs):\n",
    "            sum_square = np.sum(np.square(class_weights))\n",
    "            output[newsgroups_train.target_names[index]] = sum_square\n",
    "            \n",
    "        return [f1_score,output]\n",
    "    \n",
    "    knn_score = knn()\n",
    "    multi_nb_score = multi_naive_bayes()\n",
    "    log_reg_score, output = log_reg()\n",
    "    \n",
    "    score_d = {\"knn\":knn_score, \"multi_nb_score\": multi_nb_score, \"log_reg_score\": log_reg_score }\n",
    "    print()\n",
    "    print(\"Summary F1 Scores:\")\n",
    "    print(score_d)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "log_reg_out = P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWtXwAlOFsxr"
   },
   "source": [
    "**Notes:**\n",
    "\n",
    "\n",
    "**Why doesn't nearest neighbors work well for this problem?**\n",
    "\n",
    "- GridSearchCV determines that K=19 is the optimal value, though this is on the high end! One major drawback of KNN is that it doesn't perform well when the dimension of your data is very high (i.e. you have a lot of features). In our case here each feature vector has 26,879 features. The dimensionality of the feature space grows, but it becomes very sparse. Due to this sparsity, it becomes much easier to find a separable hyperplane because the likelihood that a training sample lies on the wrong side of the best hyperplane becomes infinitely small. However, when this highly dimensional classification is projected back to a lower dimensional space, there would be a high level of overfitting of the model to the training data. The model would learn the training data too well and not perform well on new data.\n",
    "\n",
    "<em>\"In other words, if the amount of available training data is fixed, then overfitting occurs if we keep adding dimensions. On the other hand, if we keep adding dimensions, the amount of training data needs to grow exponentially fast to maintain the same coverage and to avoid overfitting.\" </em>\n",
    "\n",
    "-- Source: https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/#comment-241\n",
    "\n",
    "- A last point here is that KNN works off distance in the feature vectors, our representation of text is a binary array of 1's and 0's as flags for the existance of words. For example the meaning associated with two sentences talking about the same topic, though the two sentences have no common words, could be considered two very different vectors on the vector space with a potentially large distance between. Hence they would not be \"neighbors\". A simple example, \"Your house is so beautiful\" vs. \"The place you live is aesthetic\".\n",
    "\n",
    "For the above reasons, we can see that KNN is not the best solution for our classification.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "**Any ideas why logistic regression doesn't work as well as Naive Bayes?**\n",
    "\n",
    "In our case here the amount of samples we train on and test on is not that large (~2000 samples). Naive bayes is built off the assumption that each feature is conditionally independent, while logistic regression is trying to calculate correlations amongst the features which then go into the odds function that ultimately determines the class. Because our feature vectors are so large, and that we are trying to represent text data, in translation it is difficult to capture all the correlations between words in lengthy somewhat sparse vectors.\n",
    "\n",
    "Generally logistic regression would perform better than naive bayes with a larger dataset size, though this does not take away from the fundamental independence assumptions that naive bayes operates on training with knowledge of the prior. \n",
    "\n",
    "-- Source https://www.quora.com/What-is-the-difference-between-logistic-regression-and-Naive-Bayes\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "**Briefly explain the relationship between the sum and the value of C.**\n",
    "\n",
    "\n",
    "As the C regularization hyper parameter goes up the sum of squares value also increases. Though there can be a slight plateau that gets hit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squares value for optimal Logistic Regression model by class.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 119.46448994413365,\n",
       " 'comp.graphics': 95.71799894836894,\n",
       " 'sci.space': 114.55234229232897,\n",
       " 'talk.religion.misc': 103.82401728947232}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Smaller values have more regularization.\n",
    "print(\"Sum of Squares value for optimal Logistic Regression model by class.\")\n",
    "log_reg_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGEjsm_uFsxr"
   },
   "source": [
    "### Part 4:\n",
    "\n",
    "Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WN51Nv4fFsxs"
   },
   "outputs": [],
   "source": [
    "def P4(bigram=False):\n",
    "    \n",
    "    #Common vectorizer\n",
    "    \n",
    "    if (bigram):\n",
    "        vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\n",
    "    else:\n",
    "        vectorizer = CountVectorizer()        \n",
    "\n",
    "    #TRAINING\n",
    "    sparse_train = vectorizer.fit_transform(train_data)\n",
    "    \n",
    "    #DEVELOPMENT\n",
    "    sparse_dev = vectorizer.transform(dev_data)\n",
    "    \n",
    "    \n",
    "    #Get vocab\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    \n",
    "    log_reg = LogisticRegression(C=0.62, multi_class=\"auto\", solver='liblinear')\n",
    "    log_reg.fit(sparse_train,train_labels)\n",
    "    class_coeffs = log_reg.coef_\n",
    "    \n",
    "    \n",
    "    top_20_features_overall = []\n",
    "    top5_by_class = {}\n",
    "    \n",
    "    for label, category in enumerate(class_coeffs,start=0):\n",
    "        \n",
    "        vals = np.array(category)\n",
    "        \n",
    "        #Get index of the top 5 greatest weights\n",
    "        top5_indices = np.argpartition(vals, -5)[-5:]\n",
    "        \n",
    "        #sorted in ascending order\n",
    "        top5_sorted_indices = top5_indices[np.argsort(vals[top5_indices])]\n",
    "\n",
    "        #This will be in descending order by heaviest weight because we prepend\n",
    "        top5_feature_names = []\n",
    "        \n",
    "        #Get the actual feature name via the index\n",
    "        for ind in top5_sorted_indices:\n",
    "            name = list(vocab.keys())[list(vocab.values()).index(ind)]\n",
    "            top5_feature_names.insert(0, name)\n",
    "                 \n",
    "        #Add to dictionary so its easy to track top 5 by class\n",
    "        top5_by_class[newsgroups_train.target_names[label]] = top5_feature_names\n",
    "        \n",
    "        #Add to total list of 20 features\n",
    "        top_20_features_overall.extend(top5_feature_names)\n",
    "\n",
    "    \n",
    "    #Dictionary to print top 20 weights table\n",
    "    top20_weights = {\"feature_name\": top_20_features_overall}\n",
    "    \n",
    "    \n",
    "    #For each class/ category get the weights for top 20 features\n",
    "    for label, category in enumerate(class_coeffs,start=0):\n",
    "        weights_by_class = []\n",
    "        for feature in top_20_features_overall:\n",
    "            weights_by_class.append(category[vocab[feature]])\n",
    "            \n",
    "        top20_weights[newsgroups_train.target_names[label]] = weights_by_class\n",
    "        \n",
    "\n",
    "    #Overall Top 20 Features, by class\n",
    "    top20_weights_df = pd.DataFrame.from_dict(top20_weights)\n",
    "            \n",
    "    #Top 5 Features in each class\n",
    "    top5_df = pd.DataFrame.from_dict(top5_by_class)\n",
    "        \n",
    "    return [top5_df, top20_weights_df, top_20_features_overall]\n",
    "\n",
    "p4_output = P4()\n",
    "p4_bigram_output = P4(bigram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 features with the largest weights for by label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>atheists</td>\n",
       "      <td>graphics</td>\n",
       "      <td>space</td>\n",
       "      <td>christians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bobby</td>\n",
       "      <td>image</td>\n",
       "      <td>orbit</td>\n",
       "      <td>christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>religion</td>\n",
       "      <td>file</td>\n",
       "      <td>nasa</td>\n",
       "      <td>blood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>atheism</td>\n",
       "      <td>3d</td>\n",
       "      <td>launch</td>\n",
       "      <td>fbi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>deletion</td>\n",
       "      <td>computer</td>\n",
       "      <td>spacecraft</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  alt.atheism comp.graphics   sci.space talk.religion.misc\n",
       "0    atheists      graphics       space         christians\n",
       "1       bobby         image       orbit          christian\n",
       "2    religion          file        nasa              blood\n",
       "3     atheism            3d      launch                fbi\n",
       "4    deletion      computer  spacecraft              order"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"5 features with the largest weights for by label\")\n",
    "p4_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for each of these 20 features by label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>atheists</td>\n",
       "      <td>0.891670</td>\n",
       "      <td>-0.093994</td>\n",
       "      <td>-0.283623</td>\n",
       "      <td>-0.696254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bobby</td>\n",
       "      <td>0.874773</td>\n",
       "      <td>-0.200736</td>\n",
       "      <td>-0.303228</td>\n",
       "      <td>-0.410370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>religion</td>\n",
       "      <td>0.845453</td>\n",
       "      <td>-0.541207</td>\n",
       "      <td>-0.698955</td>\n",
       "      <td>-0.056091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>atheism</td>\n",
       "      <td>0.838674</td>\n",
       "      <td>-0.369886</td>\n",
       "      <td>-0.392319</td>\n",
       "      <td>-0.394050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>deletion</td>\n",
       "      <td>0.830662</td>\n",
       "      <td>-0.288366</td>\n",
       "      <td>-0.311906</td>\n",
       "      <td>-0.303569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>graphics</td>\n",
       "      <td>-0.676304</td>\n",
       "      <td>1.720085</td>\n",
       "      <td>-1.172962</td>\n",
       "      <td>-0.666792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>image</td>\n",
       "      <td>-0.502427</td>\n",
       "      <td>1.186853</td>\n",
       "      <td>-0.718793</td>\n",
       "      <td>-0.406636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>file</td>\n",
       "      <td>-0.292491</td>\n",
       "      <td>1.122455</td>\n",
       "      <td>-0.720321</td>\n",
       "      <td>-0.541684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3d</td>\n",
       "      <td>-0.320770</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>-0.610625</td>\n",
       "      <td>-0.336376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>computer</td>\n",
       "      <td>0.092763</td>\n",
       "      <td>0.885192</td>\n",
       "      <td>-0.604225</td>\n",
       "      <td>-0.423103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>space</td>\n",
       "      <td>-1.121576</td>\n",
       "      <td>-1.177278</td>\n",
       "      <td>1.953924</td>\n",
       "      <td>-1.031610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>orbit</td>\n",
       "      <td>-0.369611</td>\n",
       "      <td>-0.593561</td>\n",
       "      <td>1.076410</td>\n",
       "      <td>-0.538377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>nasa</td>\n",
       "      <td>-0.497103</td>\n",
       "      <td>-0.431753</td>\n",
       "      <td>0.899588</td>\n",
       "      <td>-0.421967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>launch</td>\n",
       "      <td>-0.404216</td>\n",
       "      <td>-0.417932</td>\n",
       "      <td>0.829902</td>\n",
       "      <td>-0.296677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>spacecraft</td>\n",
       "      <td>-0.315049</td>\n",
       "      <td>-0.343612</td>\n",
       "      <td>0.792376</td>\n",
       "      <td>-0.317876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>christians</td>\n",
       "      <td>-0.647899</td>\n",
       "      <td>-0.342656</td>\n",
       "      <td>-0.439476</td>\n",
       "      <td>0.995733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>christian</td>\n",
       "      <td>-0.520263</td>\n",
       "      <td>-0.362366</td>\n",
       "      <td>-0.253790</td>\n",
       "      <td>0.983770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>blood</td>\n",
       "      <td>-0.461447</td>\n",
       "      <td>-0.100519</td>\n",
       "      <td>-0.255541</td>\n",
       "      <td>0.909824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>fbi</td>\n",
       "      <td>-0.263834</td>\n",
       "      <td>-0.234423</td>\n",
       "      <td>-0.397289</td>\n",
       "      <td>0.802745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>order</td>\n",
       "      <td>-0.693497</td>\n",
       "      <td>-0.069924</td>\n",
       "      <td>-0.131427</td>\n",
       "      <td>0.791420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name  alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
       "0      atheists     0.891670      -0.093994  -0.283623           -0.696254\n",
       "1         bobby     0.874773      -0.200736  -0.303228           -0.410370\n",
       "2      religion     0.845453      -0.541207  -0.698955           -0.056091\n",
       "3       atheism     0.838674      -0.369886  -0.392319           -0.394050\n",
       "4      deletion     0.830662      -0.288366  -0.311906           -0.303569\n",
       "5      graphics    -0.676304       1.720085  -1.172962           -0.666792\n",
       "6         image    -0.502427       1.186853  -0.718793           -0.406636\n",
       "7          file    -0.292491       1.122455  -0.720321           -0.541684\n",
       "8            3d    -0.320770       0.997879  -0.610625           -0.336376\n",
       "9      computer     0.092763       0.885192  -0.604225           -0.423103\n",
       "10        space    -1.121576      -1.177278   1.953924           -1.031610\n",
       "11        orbit    -0.369611      -0.593561   1.076410           -0.538377\n",
       "12         nasa    -0.497103      -0.431753   0.899588           -0.421967\n",
       "13       launch    -0.404216      -0.417932   0.829902           -0.296677\n",
       "14   spacecraft    -0.315049      -0.343612   0.792376           -0.317876\n",
       "15   christians    -0.647899      -0.342656  -0.439476            0.995733\n",
       "16    christian    -0.520263      -0.362366  -0.253790            0.983770\n",
       "17        blood    -0.461447      -0.100519  -0.255541            0.909824\n",
       "18          fbi    -0.263834      -0.234423  -0.397289            0.802745\n",
       "19        order    -0.693497      -0.069924  -0.131427            0.791420"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Weights for each of these 20 features by label\")\n",
    "p4_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 bigram features with the largest weights for by label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>claim that</td>\n",
       "      <td>looking for</td>\n",
       "      <td>the space</td>\n",
       "      <td>the fbi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cheers kent</td>\n",
       "      <td>in advance</td>\n",
       "      <td>the moon</td>\n",
       "      <td>ignorance is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>was just</td>\n",
       "      <td>comp graphics</td>\n",
       "      <td>sci space</td>\n",
       "      <td>cheers kent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>you are</td>\n",
       "      <td>is there</td>\n",
       "      <td>and such</td>\n",
       "      <td>but he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>look up</td>\n",
       "      <td>out there</td>\n",
       "      <td>it was</td>\n",
       "      <td>is strength</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alt.atheism  comp.graphics  sci.space talk.religion.misc\n",
       "0   claim that    looking for  the space            the fbi\n",
       "1  cheers kent     in advance   the moon       ignorance is\n",
       "2     was just  comp graphics  sci space        cheers kent\n",
       "3      you are       is there   and such             but he\n",
       "4      look up      out there     it was        is strength"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"5 bigram features with the largest weights for by label\")\n",
    "p4_bigram_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for each of these 20 bigram features by label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>claim that</td>\n",
       "      <td>0.656676</td>\n",
       "      <td>-0.216068</td>\n",
       "      <td>-0.297146</td>\n",
       "      <td>-0.157251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cheers kent</td>\n",
       "      <td>0.586131</td>\n",
       "      <td>-0.751808</td>\n",
       "      <td>-0.711163</td>\n",
       "      <td>0.557198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>was just</td>\n",
       "      <td>0.540187</td>\n",
       "      <td>-0.148648</td>\n",
       "      <td>-0.148052</td>\n",
       "      <td>-0.248535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>you are</td>\n",
       "      <td>0.502874</td>\n",
       "      <td>-0.291717</td>\n",
       "      <td>-0.510100</td>\n",
       "      <td>0.020484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>look up</td>\n",
       "      <td>0.485071</td>\n",
       "      <td>-0.182774</td>\n",
       "      <td>-0.146140</td>\n",
       "      <td>-0.122621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>looking for</td>\n",
       "      <td>-0.667762</td>\n",
       "      <td>1.173054</td>\n",
       "      <td>-0.533422</td>\n",
       "      <td>-0.609818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>in advance</td>\n",
       "      <td>-0.485167</td>\n",
       "      <td>0.875753</td>\n",
       "      <td>-0.466162</td>\n",
       "      <td>-0.444942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>comp graphics</td>\n",
       "      <td>-0.317169</td>\n",
       "      <td>0.871907</td>\n",
       "      <td>-0.399651</td>\n",
       "      <td>-0.316308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>is there</td>\n",
       "      <td>-0.366871</td>\n",
       "      <td>0.803858</td>\n",
       "      <td>-0.498207</td>\n",
       "      <td>-0.281318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>out there</td>\n",
       "      <td>-0.289680</td>\n",
       "      <td>0.800954</td>\n",
       "      <td>-0.509030</td>\n",
       "      <td>-0.293876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>the space</td>\n",
       "      <td>-0.282021</td>\n",
       "      <td>-0.565383</td>\n",
       "      <td>0.920124</td>\n",
       "      <td>-0.289090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>the moon</td>\n",
       "      <td>-0.367225</td>\n",
       "      <td>-0.517982</td>\n",
       "      <td>0.868239</td>\n",
       "      <td>-0.221882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>sci space</td>\n",
       "      <td>-0.275694</td>\n",
       "      <td>-0.347238</td>\n",
       "      <td>0.657469</td>\n",
       "      <td>-0.237054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>and such</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-0.356624</td>\n",
       "      <td>0.620775</td>\n",
       "      <td>-0.229982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>it was</td>\n",
       "      <td>-0.219900</td>\n",
       "      <td>-0.326863</td>\n",
       "      <td>0.551729</td>\n",
       "      <td>-0.336890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>the fbi</td>\n",
       "      <td>-0.142157</td>\n",
       "      <td>-0.226372</td>\n",
       "      <td>-0.314768</td>\n",
       "      <td>0.583614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>ignorance is</td>\n",
       "      <td>-0.177281</td>\n",
       "      <td>-0.193558</td>\n",
       "      <td>-0.158872</td>\n",
       "      <td>0.573172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>cheers kent</td>\n",
       "      <td>0.586131</td>\n",
       "      <td>-0.751808</td>\n",
       "      <td>-0.711163</td>\n",
       "      <td>0.557198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>but he</td>\n",
       "      <td>-0.209005</td>\n",
       "      <td>-0.234044</td>\n",
       "      <td>-0.150030</td>\n",
       "      <td>0.527691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>is strength</td>\n",
       "      <td>-0.131256</td>\n",
       "      <td>-0.168221</td>\n",
       "      <td>-0.143599</td>\n",
       "      <td>0.485029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_name  alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
       "0      claim that     0.656676      -0.216068  -0.297146           -0.157251\n",
       "1     cheers kent     0.586131      -0.751808  -0.711163            0.557198\n",
       "2        was just     0.540187      -0.148648  -0.148052           -0.248535\n",
       "3         you are     0.502874      -0.291717  -0.510100            0.020484\n",
       "4         look up     0.485071      -0.182774  -0.146140           -0.122621\n",
       "5     looking for    -0.667762       1.173054  -0.533422           -0.609818\n",
       "6      in advance    -0.485167       0.875753  -0.466162           -0.444942\n",
       "7   comp graphics    -0.317169       0.871907  -0.399651           -0.316308\n",
       "8        is there    -0.366871       0.803858  -0.498207           -0.281318\n",
       "9       out there    -0.289680       0.800954  -0.509030           -0.293876\n",
       "10      the space    -0.282021      -0.565383   0.920124           -0.289090\n",
       "11       the moon    -0.367225      -0.517982   0.868239           -0.221882\n",
       "12      sci space    -0.275694      -0.347238   0.657469           -0.237054\n",
       "13       and such    -0.215840      -0.356624   0.620775           -0.229982\n",
       "14         it was    -0.219900      -0.326863   0.551729           -0.336890\n",
       "15        the fbi    -0.142157      -0.226372  -0.314768            0.583614\n",
       "16   ignorance is    -0.177281      -0.193558  -0.158872            0.573172\n",
       "17    cheers kent     0.586131      -0.751808  -0.711163            0.557198\n",
       "18         but he    -0.209005      -0.234044  -0.150030            0.527691\n",
       "19    is strength    -0.131256      -0.168221  -0.143599            0.485029"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Weights for each of these 20 bigram features by label\")\n",
    "p4_bigram_output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cY67F-tXFsxt"
   },
   "source": [
    "**Notes:**\n",
    "\n",
    "Some surprising things are that comp.graphics and sci space actual are bigram feature names that fall under the top 5 respective to their category! \n",
    "\n",
    "In addition the bigram model seems some features that are rather generic. Features like \"you are\" and \"is there\". I would think will make the bigram model worse than the unigram given that some of these \"stop word\" equivalent features are not cleaned out.\n",
    "\n",
    "In addition we see the feature \"cheers kent\", come up as a popular feature in both the \"alt.atheism\" and \"talk.religion.misc\" classes. Looking up the phrase, it had a rather innappropriate connotation so it was shocking to see it come up as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVWhSJHHFsxu"
   },
   "source": [
    "### Part 5:\n",
    "\n",
    "Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "For reference, I was able to improve dev F1 by 2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Preprocessor\n",
    "# test_str = 'Box A contained 3 red and 5 white balls, while Box B contained 4 red and 2 blue balls.'\n",
    "# better_preprocessor(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7gS3cGpFsxv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom processed vocab Length: 14283\n",
      "\n",
      "F1 Score: 0.7086198003420923\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Custom Process + stop words cleaned vocab Length: 14111\n",
      "\n",
      "F1 Score: 0.71602985695681\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "No Process Vocab Length: 26879\n",
      "\n",
      "F1 Score: 0.6877785342087575\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Various levels of processing on a given string\n",
    "def better_preprocessor(s):\n",
    "\n",
    "    #lower case everything (Actually lowered the F1 score so this is commented out)\n",
    "#     s = s.lower()\n",
    "\n",
    "    #get rid of digits, replace with empty string\n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    \n",
    "    #Get rid of the following punctuation:  [!#$%&()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "    s = s.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "               \n",
    "    #Removes leading and ending white spaces               \n",
    "    s = s.strip()\n",
    "\n",
    "    #Stemming aims to reduce words to their base, PorterStemmer removes common morphological and infexional endings\n",
    "    stemmer= PorterStemmer()\n",
    "    s=word_tokenize(s)\n",
    "    stems = []\n",
    "    for word in s:\n",
    "        cur_stem = stemmer.stem(word)\n",
    "        stems.append(cur_stem)\n",
    "        \n",
    "    s = \" \".join(stems)\n",
    "        \n",
    "                 \n",
    "    #Lemmatization uses lexical knowledge bases to get the correct base forms of words.\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    s=word_tokenize(s)\n",
    "    base_words = []\n",
    "    for word in s:\n",
    "        lem = lemmatizer.lemmatize(word)\n",
    "        base_words.append(lem)\n",
    "        \n",
    "    s = \" \".join(base_words)\n",
    "    \n",
    "    \n",
    "#     Truncating long words to be a max of 5 characters\n",
    "    s=word_tokenize(s)\n",
    "    trunc_words = []\n",
    "    for word in s:\n",
    "        trunc_words.append(word[0:5])\n",
    "    s = \" \".join(trunc_words)        \n",
    "\n",
    "    return s\n",
    "\n",
    "def getF1Scores(vect):\n",
    "    \n",
    "    #TRAINING\n",
    "    sparse_train = vect.fit_transform(train_data)\n",
    "\n",
    "    #DEVELOPMENT\n",
    "    sparse_dev = vect.transform(dev_data)\n",
    "    log_reg = LogisticRegression(C = .62, multi_class=\"auto\")\n",
    "    log_reg.fit(sparse_train,train_labels)\n",
    "    train_predict = log_reg.predict(sparse_dev)\n",
    "    f1_score = metrics.f1_score(dev_labels, train_predict, average=\"weighted\")\n",
    "    print()\n",
    "    print(\"F1 Score: {0}\".format(f1_score))      \n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def P5():\n",
    "    processed_vectorizer = CountVectorizer(\n",
    "        lowercase=True,\n",
    "        preprocessor = better_preprocessor)\n",
    "    \n",
    "    processed_train = processed_vectorizer.fit_transform(train_data)    \n",
    "    processed_vocab = processed_vectorizer.vocabulary_\n",
    "    print(\"Custom processed vocab Length: {0}\".format(len(processed_vocab.keys())))  \n",
    "    getF1Scores(processed_vectorizer)\n",
    "    \n",
    "    processed_stop_vectorizer = CountVectorizer(\n",
    "        lowercase=True,\n",
    "        preprocessor = better_preprocessor,\n",
    "        stop_words='english',\n",
    "        analyzer = \"word\")\n",
    "\n",
    "    processed_stop_train = processed_stop_vectorizer.fit_transform(train_data)    \n",
    "    processed_stop_vocab = processed_stop_vectorizer.vocabulary_\n",
    "    print(\"Custom Process + stop words cleaned vocab Length: {0}\".format(len(processed_stop_vocab.keys())))\n",
    "    getF1Scores(processed_stop_vectorizer)\n",
    "\n",
    "    no_process_vectorizer = CountVectorizer(lowercase=True)\n",
    "    no_process_train = no_process_vectorizer.fit_transform(train_data)    \n",
    "    no_process_vocab = no_process_vectorizer.vocabulary_    \n",
    "    print(\"No Process Vocab Length: {0}\".format(len(no_process_vocab.keys())))\n",
    "    getF1Scores(no_process_vectorizer)\n",
    "                 \n",
    "    print()\n",
    "    return [processed_vectorizer, processed_stop_vectorizer, no_process_vectorizer]\n",
    "\n",
    "process_levels = P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the custom pre-processor the vocabulary is **12,596 words smaller**. When applying the preprocessor and getting rid of stop words we drop **12,768 words.**\n",
    "\n",
    "F1 Score Increase (Custom + Stop Words) - (No Process) = **0.02825**\n",
    "\n",
    "F1 Score Increase (Custom) - (No Process) = **0.02084**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uy-WITbNFsxw"
   },
   "source": [
    "### Part 6:\n",
    "\n",
    "The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.015 (the default is .0001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6ho31SrFsxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 - Non zero learned weights\n",
      "count: 1352\n",
      "Accuracy - L1 LogReg Model: 0.7026627218934911\n",
      "\n",
      "L2 - Non zero learned weights\n",
      "count: 107516\n",
      "Accuracy - L2 LogReg Model: 0.7056213017751479\n",
      "\n",
      "\n",
      "L2 with smaller vocab - Non zero learned weights\n",
      "count: 3164\n",
      "Accuracy of smaller vocab L2 LogReg Model: 0.6789940828402367\n",
      "\n",
      "\n",
      "C Vals (for L1): [0.01, 0.02, 0.05, 0.08, 0.1, 0.175, 0.25, 0.35, 0.5, 0.75, 1, 3, 5, 7, 10]\n",
      "\n",
      "Vocab Sizes: [17, 43, 125, 179, 213, 330, 446, 587, 733, 936, 1115, 1958, 2472, 3062, 3544]\n",
      "\n",
      "Accuracies: [0.4807692307692308, 0.5946745562130178, 0.6316568047337278, 0.6760355029585798, 0.6923076923076923, 0.6834319526627219, 0.6834319526627219, 0.6878698224852071, 0.6834319526627219, 0.6775147928994083, 0.6863905325443787, 0.6923076923076923, 0.7026627218934911, 0.6952662721893491, 0.6982248520710059]\n",
      "\n",
      "\n",
      "The vocab sizes are varied by L1 regularization hyper-param C\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def P6():\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "\n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    train = vectorizer.fit_transform(train_data)\n",
    "    sparse_dev = vectorizer.transform(dev_data)\n",
    "    vocab = vectorizer.vocabulary_\n",
    "\n",
    "    def part1():\n",
    "        # L1 Loss - Log Reg\n",
    "        l1_log_reg = LogisticRegression(penalty='l1',C=.62, multi_class=\"auto\", solver='liblinear')\n",
    "        l1_log_reg.fit(train,train_labels)\n",
    "        l1_train_predict = l1_log_reg.predict(sparse_dev)\n",
    "        accuracy = metrics.accuracy_score(dev_labels, l1_train_predict)   \n",
    "        l1_coeffs = l1_log_reg.coef_.flatten()\n",
    "\n",
    "        # L1 - Non zero learned weights\n",
    "        l1_count = sum(l1_coeffs.astype(bool))\n",
    "        print(\"L1 - Non zero learned weights\")\n",
    "        print(\"count: {0}\".format(l1_count))\n",
    "        print(\"Accuracy - L1 LogReg Model: {0}\".format(accuracy))        \n",
    "        print()\n",
    "\n",
    "\n",
    "        # L2 Loss - Log Reg\n",
    "        l2_log_reg = LogisticRegression(penalty='l2',C=.62, multi_class=\"auto\", solver='liblinear')\n",
    "        l2_log_reg.fit(train,train_labels)\n",
    "        l2_train_predict = l2_log_reg.predict(sparse_dev)\n",
    "        accuracy = metrics.accuracy_score(dev_labels, l2_train_predict)           \n",
    "        l2_coeffs = l2_log_reg.coef_.flatten()\n",
    "\n",
    "        # L2 - Non zero learned weights\n",
    "        l2_count = sum(l2_coeffs.astype(bool))\n",
    "        print(\"L2 - Non zero learned weights\")\n",
    "        print(\"count: {0}\".format(l2_count ))\n",
    "        print(\"Accuracy - L2 LogReg Model: {0}\".format(accuracy))        \n",
    "        print()\n",
    "        \n",
    "        return l1_log_reg.coef_\n",
    "        \n",
    "        \n",
    "    def part2(l1_coeffs, tol_val=None):\n",
    "        \n",
    "        list_nonzero = []\n",
    "\n",
    "        #All the indicies with non-zero values\n",
    "        [list_nonzero.extend(list(np.nonzero(class_coef)[0])) for class_coef in l1_coeffs]        \n",
    "        \n",
    "        #Get only unique ones\n",
    "        smaller_feature_indices = list(set(list(list_nonzero)))\n",
    "        \n",
    "        \n",
    "        # Smaller vocab\n",
    "        feat_names = vectorizer.get_feature_names()\n",
    "        smaller_vocab = [feat_names[slot] for slot in smaller_feature_indices]\n",
    "    \n",
    "        small_vectorizer = CountVectorizer(lowercase=True, vocabulary=smaller_vocab)\n",
    "        small_train = small_vectorizer.fit_transform(train_data)\n",
    "        small_sparse_dev = small_vectorizer.transform(dev_data)\n",
    "        small_vocab = small_vectorizer.vocabulary_\n",
    "        \n",
    "        \n",
    "        if not tol_val:\n",
    "            l2_small = LogisticRegression(penalty='l2',C=.62, multi_class=\"auto\", solver='liblinear')\n",
    "        else:\n",
    "            l2_small = LogisticRegression(penalty='l2',C=.62\n",
    "                                          , multi_class=\"auto\", solver='liblinear', tol=tol_val)            \n",
    "            \n",
    "            \n",
    "        l2_small.fit(small_train,train_labels)      \n",
    "        l2_small_predict = l2_small.predict(small_sparse_dev)\n",
    "        accuracy = metrics.accuracy_score(dev_labels, l2_small_predict)            \n",
    "        \n",
    "        if not tol_val:        \n",
    "            print()\n",
    "            print (\"L2 with smaller vocab - Non zero learned weights\")\n",
    "            print(\"count: {0}\".format(np.count_nonzero(l2_small.coef_)))\n",
    "            print(\"Accuracy of smaller vocab L2 LogReg Model: {0}\".format(accuracy))\n",
    "            print()\n",
    "        \n",
    "        vocab_size = len(smaller_vocab)\n",
    "        \n",
    "        return [vocab_size, accuracy]\n",
    "\n",
    "\n",
    "    def part3(tol_val =.015):\n",
    "        c_vals = [0.01, 0.02, 0.05, 0.08, 0.1, 0.175, 0.25, 0.35, 0.5, 0.75, 1, 3, 5, 7, 10]\n",
    "        vocab_sizes = []\n",
    "        accuracies = []  \n",
    "        \n",
    "        for c_val in c_vals:\n",
    "\n",
    "            vectorizer = CountVectorizer()\n",
    "            train = vectorizer.fit_transform(train_data)\n",
    "            l1_log_reg = LogisticRegression(penalty='l1', C=c_val, tol=tol_val, solver = 'liblinear')\n",
    "            l1_log_reg.fit(train, train_labels)\n",
    "            \n",
    "            vocab_size,accuracy=part2(l1_log_reg.coef_,tol_val)\n",
    "            \n",
    "            vocab_sizes.append(vocab_size)\n",
    "            accuracies.append(accuracy)       \n",
    "            \n",
    "        print()\n",
    "        print(\"C Vals (for L1): {0}\".format(c_vals))\n",
    "        print()\n",
    "        print(\"Vocab Sizes: {0}\".format(vocab_sizes))\n",
    "        print()        \n",
    "        print(\"Accuracies: {0}\".format(accuracies))\n",
    "        print()        \n",
    "        print()        \n",
    "        print(\"The vocab sizes are varied by L1 regularization hyper-param C\")\n",
    "        print()\n",
    "        print()   \n",
    "        \n",
    "        \n",
    "        plt.figure(0)\n",
    "        plt.plot(vocab_sizes, accuracies, 'go--')\n",
    "        plt.xlabel(\"Vocabulary Size\")\n",
    "        plt.ylabel(\"Accuracies\")\n",
    "        plt.title(\"Accuracies of L2 Logistic Regression vs. Vocabulary Size\")\n",
    "        \n",
    "        plt.figure(1)\n",
    "        plt.plot(c_vals, accuracies, 'ro--')\n",
    "        plt.xlabel(\"C Vals\")\n",
    "        plt.ylabel(\"Accuracies\")\n",
    "        plt.title(\"Accuracies of L2 Logistic Regression vs. C_vals\")        \n",
    "                \n",
    "    \n",
    "    part1 = part1()\n",
    "    part2(part1)\n",
    "    part3()\n",
    "    \n",
    "P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmrHf8AMFsxz"
   },
   "source": [
    "### Part 7:\n",
    "\n",
    "Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
    "\n",
    "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "\n",
    "maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vwU_9t2Fsx0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDVectorizer F1 Scores:0.7597662427853104\n",
      "\n",
      "\n",
      "Correct Label: talk.religion.misc\n",
      "Predicted Label: comp.graphics\n",
      "\n",
      "R-Ratio: 929.3569346206551\n",
      "\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      "Correct Label: talk.religion.misc\n",
      "Predicted Label: comp.graphics\n",
      "\n",
      "R-Ratio: 325.00381439841027\n",
      "\n",
      "Can anyone provide me a ftp site where I can obtain a online version\n",
      "of the Book of Mormon. Please email the internet address if possible.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      "Correct Label: alt.atheism\n",
      "Predicted Label: talk.religion.misc\n",
      "\n",
      "R-Ratio: 287.3067229495238\n",
      "\n",
      "\n",
      "The 24 children were, of course, killed by a lone gunman in a second story\n",
      "window, who fired eight bullets in the space of two seconds...\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P7():\n",
    "    \n",
    "    log_reg = LogisticRegression(C=100, solver=\"liblinear\", multi_class=\"auto\")\n",
    "\n",
    "    #Common vectorizer\n",
    "    tfid_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    #TRAINING\n",
    "    tfid_train = tfid_vectorizer.fit_transform(train_data)\n",
    "    \n",
    "    #DEVELOPMENT\n",
    "    tfid_sparse_dev = tfid_vectorizer.transform(dev_data)\n",
    "\n",
    "    \n",
    "    log_reg.fit(tfid_train,train_labels)  \n",
    "    tfid_predict = log_reg.predict(tfid_sparse_dev)\n",
    "    f1_score_tfid = metrics.f1_score(dev_labels, tfid_predict, average=\"weighted\")    \n",
    "    print(\"TFIDVectorizer F1 Scores:{0}\".format(f1_score_tfid))\n",
    "    \n",
    "    predicted_probs = log_reg.predict_proba(tfid_sparse_dev)\n",
    "    \n",
    "    r_ratios = []\n",
    "    max_probs = []\n",
    "    correct_probs = []\n",
    "    correct_label_names=[]\n",
    "    predicted_label_names=[]    \n",
    "    documents = []\n",
    "    \n",
    "    for i in range(0,len(dev_labels)):\n",
    "        \n",
    "        max_prob = np.max(predicted_probs[i])\n",
    "        correct_prob = predicted_probs[i][dev_labels[i]]\n",
    "        \n",
    "        predicted_label_name = newsgroups_train.target_names[np.argmax(predicted_probs[i])]\n",
    "        correct_label_name = newsgroups_train.target_names[dev_labels[i]]\n",
    "        \n",
    "        r_ratio = max_prob/correct_prob\n",
    "        \n",
    "        \n",
    "        document = dev_data[i]\n",
    "\n",
    "        max_probs.append(max_prob)\n",
    "        correct_probs.append(correct_prob)        \n",
    "        r_ratios.append(r_ratio)\n",
    "        correct_label_names.append(correct_label_name)\n",
    "        predicted_label_names.append(predicted_label_name)\n",
    "        documents.append(document)\n",
    "\n",
    "        \n",
    "    d = {'correct_label_name': correct_label_names, 'predicted_label_name':predicted_label_names, 'r_ratio': r_ratios,\n",
    "         'max_prob': max_probs, 'correct_prob': correct_probs, 'document':documents}\n",
    "\n",
    "    stats_table = pd.DataFrame(data=d)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    entry_count= 0\n",
    "    for index, row in stats_table.sort_values(by=['r_ratio'], ascending=False).iterrows():\n",
    "        \n",
    "        if entry_count == 3:\n",
    "            break\n",
    "\n",
    "        print()\n",
    "        print(\"Correct Label: \"+ row['correct_label_name'])\n",
    "        print(\"Predicted Label: \" + row['predicted_label_name'])\n",
    "        print()\n",
    "        print(\"R-Ratio: {0}\".format(row['r_ratio']))    \n",
    "        print()\n",
    "        print(row[\"document\"])\n",
    "        print()\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print()\n",
    "        entry_count+=1\n",
    "        \n",
    "        \n",
    "    conf_mat = confusion_matrix(correct_label_names, predicted_label_names)        \n",
    "\n",
    "    plt.show()        \n",
    "        \n",
    "        \n",
    "    return [stats_table, conf_mat]\n",
    "\n",
    "\n",
    "p7_df, p7_conf_mat = P7()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_label_name</th>\n",
       "      <th>predicted_label_name</th>\n",
       "      <th>r_ratio</th>\n",
       "      <th>max_prob</th>\n",
       "      <th>correct_prob</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>929.356935</td>\n",
       "      <td>0.993719</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>I am pleased to announce that a *revised versi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>325.003814</td>\n",
       "      <td>0.979744</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>Can anyone provide me a ftp site where I can o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>287.306723</td>\n",
       "      <td>0.695245</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>\\nThe 24 children were, of course, killed by a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>212.067692</td>\n",
       "      <td>0.974435</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>In &lt;1ren9a$94q@morrow.stanford.edu&gt; salem@pang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>159.076241</td>\n",
       "      <td>0.964994</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>With the Southern Baptist Convention convening...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     correct_label_name predicted_label_name     r_ratio  max_prob  \\\n",
       "215  talk.religion.misc        comp.graphics  929.356935  0.993719   \n",
       "665  talk.religion.misc        comp.graphics  325.003814  0.979744   \n",
       "607         alt.atheism   talk.religion.misc  287.306723  0.695245   \n",
       "655  talk.religion.misc          alt.atheism  212.067692  0.974435   \n",
       "287         alt.atheism   talk.religion.misc  159.076241  0.964994   \n",
       "\n",
       "     correct_prob                                           document  \n",
       "215      0.001069  I am pleased to announce that a *revised versi...  \n",
       "665      0.003015  Can anyone provide me a ftp site where I can o...  \n",
       "607      0.002420  \\nThe 24 children were, of course, killed by a...  \n",
       "655      0.004595  In <1ren9a$94q@morrow.stanford.edu> salem@pang...  \n",
       "287      0.006066  With the Southern Baptist Convention convening...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p7_df.sort_values(by=['r_ratio'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZmIPwExFsx1"
   },
   "source": [
    "**Notes:**\n",
    "\n",
    "**TfidfVectorizer vs. CountVectorizer**\n",
    "\n",
    "These are both are methods to convert text data into vectors of numerical values as representations for the text. \n",
    "\n",
    "With CountVectorizer we count the number of times a word appears in a document which can result in a bias towards the most frequent words, as their slot on the vector would be larger than the average word.(i.e, a document that is all the same word would strongly bias the data). As a result of the bias, CountVectorizer can overlook more rare words which could actually be very relevant for classification.\n",
    "\n",
    "With TfidfVectorizer we look at an overall document weightage of a word. The inverse document frequency calculation adjusts for the scenarios where some words appear more frequently in general. For example, even though words like we, and the appear often in documents, since it appears often in a bunch of documents, they don't provide much useful information about what makes a document unique. Overall this helps us penalize super frequent words, and rather weights the word counts based on how often they appear throughout the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Confusion Matrix Visualization: https://github.com/matplotlib/matplotlib/issues/14751\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(p7_conf_mat)\n",
    "\n",
    "ax.set_xticks(np.arange(len(newsgroups_train.target_names)))\n",
    "ax.set_yticks(np.arange(len(newsgroups_train.target_names)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(newsgroups_train.target_names)\n",
    "ax.set_yticklabels(newsgroups_train.target_names)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    for j in range(len(newsgroups_train.target_names)):\n",
    "        text = ax.text(j, i, p7_conf_mat[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"red\")\n",
    "\n",
    "ax.set_ylim(len(p7_conf_mat)-0.5, -0.5)\n",
    "ax.set_xlabel(\"Predict Labels\")\n",
    "ax.set_ylabel(\"True Labels\")\n",
    "ax.set_ylim(len(p7_conf_mat)-0.5, -0.5)\n",
    "ax.grid(False)\n",
    "ax.set_title(\"Confusion Matrix by Class \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each column is just the count values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>r_ratio</th>\n",
       "      <th>max_prob</th>\n",
       "      <th>correct_prob</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct_label_name</th>\n",
       "      <th>predicted_label_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"3\" valign=\"top\">alt.atheism</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sci.space</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"3\" valign=\"top\">comp.graphics</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sci.space</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"3\" valign=\"top\">sci.space</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"3\" valign=\"top\">talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sci.space</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         r_ratio  max_prob  correct_prob  \\\n",
       "correct_label_name predicted_label_name                                    \n",
       "alt.atheism        comp.graphics              11        11            11   \n",
       "                   sci.space                  19        19            19   \n",
       "                   talk.religion.misc         33        33            33   \n",
       "comp.graphics      alt.atheism                 5         5             5   \n",
       "                   sci.space                  10        10            10   \n",
       "                   talk.religion.misc          2         2             2   \n",
       "sci.space          alt.atheism                10        10            10   \n",
       "                   comp.graphics              20        20            20   \n",
       "                   talk.religion.misc          4         4             4   \n",
       "talk.religion.misc alt.atheism                31        31            31   \n",
       "                   comp.graphics              10        10            10   \n",
       "                   sci.space                   5         5             5   \n",
       "\n",
       "                                         document  \n",
       "correct_label_name predicted_label_name            \n",
       "alt.atheism        comp.graphics               11  \n",
       "                   sci.space                   19  \n",
       "                   talk.religion.misc          33  \n",
       "comp.graphics      alt.atheism                  5  \n",
       "                   sci.space                   10  \n",
       "                   talk.religion.misc           2  \n",
       "sci.space          alt.atheism                 10  \n",
       "                   comp.graphics               20  \n",
       "                   talk.religion.misc           4  \n",
       "talk.religion.misc alt.atheism                 31  \n",
       "                   comp.graphics               10  \n",
       "                   sci.space                    5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Each column is just the count values\")\n",
    "p7_df.query('correct_label_name != predicted_label_name').groupby(by=['correct_label_name','predicted_label_name']).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potential Issues**\n",
    "\n",
    "Looking at our confusion matrix, we can see that there almost an even number of times in both ways (30-ish) where the model is confusing \"religion\" and \"atheism\". In addition we construct a grouped dataframe above for all the incorrect labelled scenarios.\n",
    "\n",
    "We can see that across the board 'atheism' is the class category that has the highest incrorrect predictions. So lets take a look at some of the documents that have been misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_label_name</th>\n",
       "      <th>predicted_label_name</th>\n",
       "      <th>r_ratio</th>\n",
       "      <th>max_prob</th>\n",
       "      <th>correct_prob</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>287.306723</td>\n",
       "      <td>0.695245</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>\\nThe 24 children were, of course, killed by a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>159.076241</td>\n",
       "      <td>0.964994</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>With the Southern Baptist Convention convening...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>85.088278</td>\n",
       "      <td>0.579454</td>\n",
       "      <td>0.006810</td>\n",
       "      <td>I think that _The_Transcedental_Temptation_, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>82.917537</td>\n",
       "      <td>0.795952</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>\\nThe CLIPPER initiative is an announcement by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>57.294291</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>TEST-- \\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>50.698837</td>\n",
       "      <td>0.942219</td>\n",
       "      <td>0.018585</td>\n",
       "      <td>As requested, here are some addresses of sourc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>49.561228</td>\n",
       "      <td>0.902747</td>\n",
       "      <td>0.018215</td>\n",
       "      <td>\\nThe \"R Us\" thing is trademarked.  I don't kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>48.570829</td>\n",
       "      <td>0.965774</td>\n",
       "      <td>0.019884</td>\n",
       "      <td>MC&gt; Theory of Creationism: MY theistic view of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>33.250726</td>\n",
       "      <td>0.652600</td>\n",
       "      <td>0.019627</td>\n",
       "      <td>\\n\\n\\nMuch of the Haight-Ashbury crowd probabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>31.328611</td>\n",
       "      <td>0.828428</td>\n",
       "      <td>0.026443</td>\n",
       "      <td>\\nYes, unless the observer is at rest with res...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    correct_label_name predicted_label_name     r_ratio  max_prob  \\\n",
       "607        alt.atheism   talk.religion.misc  287.306723  0.695245   \n",
       "287        alt.atheism   talk.religion.misc  159.076241  0.964994   \n",
       "236        alt.atheism            sci.space   85.088278  0.579454   \n",
       "177        alt.atheism            sci.space   82.917537  0.795952   \n",
       "209        alt.atheism            sci.space   57.294291  0.768041   \n",
       "465        alt.atheism   talk.religion.misc   50.698837  0.942219   \n",
       "52         alt.atheism   talk.religion.misc   49.561228  0.902747   \n",
       "181        alt.atheism   talk.religion.misc   48.570829  0.965774   \n",
       "141        alt.atheism   talk.religion.misc   33.250726  0.652600   \n",
       "484        alt.atheism            sci.space   31.328611  0.828428   \n",
       "\n",
       "     correct_prob                                           document  \n",
       "607      0.002420  \\nThe 24 children were, of course, killed by a...  \n",
       "287      0.006066  With the Southern Baptist Convention convening...  \n",
       "236      0.006810  I think that _The_Transcedental_Temptation_, b...  \n",
       "177      0.009599  \\nThe CLIPPER initiative is an announcement by...  \n",
       "209      0.013405                                        TEST-- \\n\\n  \n",
       "465      0.018585  As requested, here are some addresses of sourc...  \n",
       "52       0.018215  \\nThe \"R Us\" thing is trademarked.  I don't kn...  \n",
       "181      0.019884  MC> Theory of Creationism: MY theistic view of...  \n",
       "141      0.019627  \\n\\n\\nMuch of the Haight-Ashbury crowd probabl...  \n",
       "484      0.026443  \\nYes, unless the observer is at rest with res...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atheism_sorted = p7_df.query('correct_label_name != predicted_label_name & correct_label_name == \\'alt.atheism\\'').sort_values(by=['r_ratio'], ascending=False)\n",
    "atheism_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.religion.misc\n",
      "\n",
      "\n",
      "The 24 children were, of course, killed by a lone gunman in a second story\n",
      "window, who fired eight bullets in the space of two seconds...\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "talk.religion.misc\n",
      "\n",
      "With the Southern Baptist Convention convening this June to consider\n",
      "the charges that Freemasonry is incompatible with christianity, I thought\n",
      "the following quotes by Mr. James Holly, the Anti-Masonic Flag Carrier,\n",
      "would amuse you all...\n",
      "\n",
      "     The following passages are exact quotes from \"The Southern \n",
      "Baptist Convention and Freemasonry\" by James L. Holly, M.D., President\n",
      "of Mission and Ministry To Men, Inc., 550 N 10th St., Beaumont, TX \n",
      "77706. \n",
      " \n",
      "     The inside cover of the book states: \"Mission & Ministry to Men, \n",
      "Inc. hereby grants permission for the reproduction of part or all of \n",
      "this booklet with two provisions: one, the material is not changed and\n",
      "two, the source is identified.\" I have followed these provisions. \n",
      "  \n",
      "     \"Freemasonry is one of the allies of the Devil\" Page iv. \n",
      " \n",
      "     \"The issue here is not moderate or conservative, the issue is God\n",
      "and the Devil\" Page vi.\" \n",
      " \n",
      "     \"It is worthwhile to remember that the formulators of public \n",
      "school education in America were Freemasons\" Page 29. \n",
      " \n",
      "     \"Jesus Christ never commanded toleration as a motive for His \n",
      "disciples, and toleration is the antithesis of the Christian message.\"\n",
      "Page 30. \n",
      " \n",
      "     \"The central dynamic of the Freemason drive for world unity \n",
      "through fraternity, liberty and equality is toleration. This is seen \n",
      "in the writings of the 'great' writers of Freemasonry\". Page 31. \n",
      " \n",
      "     \"He [Jesus Christ] established the most sectarian of all possible \n",
      "faiths.\" Page 37. \n",
      " \n",
      "     \"For narrowness and sectarianism, there is no equal to the Lord \n",
      "Jesus Christ\". Page 40. \n",
      " \n",
      "     \"What seems so right in the interest of toleration and its \n",
      "cousins-liberty, equality and fraternity-is actually one of the \n",
      "subtlest lies of the 'father of lies.'\" Page 40. \n",
      " \n",
      "     \"The Southern Baptist Convention has many churches which were \n",
      "founded in the Lodge and which have corner stones dedicated by the \n",
      "Lodge. Each of these churches should hold public ceremonies of \n",
      "repentance and of praying the blood and the Name of the Lord Jesus \n",
      "Christ over the church and renouncing the oaths taken at the \n",
      "dedication of the church and/or building.\" Page 53-54.  \n",
      " \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "sci.space\n",
      "\n",
      "I think that _The_Transcedental_Temptation_, by Paul Kurtz, has a good\n",
      "section on the origins of Mormonism you might want to look at.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "sci.space\n",
      "\n",
      "\n",
      "The CLIPPER initiative is an announcement by Clinton that all the \n",
      "\"secure\" voice phones will use the same crypto chip, as a de-facto\n",
      "government standard.  Problem is, the government is admitting that\n",
      "they hold the keys to break the code easily, and the Justice department\n",
      "will be using the keys to listen in on \"illegal activities.\"  Many\n",
      "people are really scared about such an initiative because it is\n",
      "a major step towards outlawing real crypto protection on things\n",
      "like email if you read the press release.  The project was developed\n",
      "by NSA and given to NIST.  It uses two keys S1 and S2 that the\n",
      "government claims are needed to break the code.  They claim that\n",
      "these keys will be handed to two different companies, and when they\n",
      "get a warrant to do a wiretap (the chip is nicknamed the wiretap chip),\n",
      "they have to get the keys from both companies.  People have poked holes\n",
      "through and through the press release official version and shown how\n",
      "it is nowhere near as nice as it sounds, and I have given the simplified\n",
      "version.  People over on sci.crypt are really scared about this\n",
      "proposal it seems.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "sci.space\n",
      "\n",
      "TEST-- \n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "entry_count = 0\n",
    "for index, row in atheism_sorted.iterrows():\n",
    "    if entry_count == 5:\n",
    "        break\n",
    "    print(row[\"predicted_label_name\"])\n",
    "    print()\n",
    "    print(row[\"document\"])       \n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")    \n",
    "    entry_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately some solutions to improve the model, could begin with applying preprocessing before running our TfidVectorizer and tokenizing our words. In theory we could play with all the various pre processing steps that I have included in Problem #5, this should help! In addition we can adjust the vocab size with minimum and/or maximum thresholds for word frequency using \"min_df\" and \"max_df\" as an option within TfidfVectorizer.\n",
    "\n",
    "Our top 3 examples and the set of top 10 analyzed \"atheism\" documents that were incorrectly classifed, have the largest R ratios therefore are the cases where the model very confidentally made the wrong prediction. \n",
    "\n",
    "Interestingly enough in the cases that the correct label was **talk.religion.misc** but was predicted as **comp.graphics** both have what we can call \"proper nouns\" such as the words \"Mormon\", \"ftp\", \"online\", and \"internet\". For the \"atheism\" documents, we see similar patterns that the prediction was wrong because the documents had words/phrases such as \"Jesus Christ\", \"The_Transcedental_Temptation\", \"government\". These are unique words that might not be getting weighted appropriately. A possible solution is doing some analysis to figure out what these top unique words are, and increasing the weightage on these type of words relative to the correct class category. This would happen during the preprocessing and training stage and using our computed set list of proper nouns by category. Padding those words in to the document to increase the frequency of that words appearance. This way the model learns the weights with a slight bias, but nudged in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMaqe8c5Fsx2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "firstname_lastname_p2.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/MIDS-W207/Master/blob/master/Projects/firstname_lastname_p2.ipynb",
     "timestamp": 1559779272103
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
